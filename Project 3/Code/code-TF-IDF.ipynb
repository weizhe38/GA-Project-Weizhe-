{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07be20e0",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 15px; height: 80px\">\n",
    "\n",
    "# Project 3: Web APIs & NLP - TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed2db81",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e543abb7",
   "metadata": {},
   "source": [
    "Current classifying model using straightforward keywords such as ‘bootcamp’ and ‘coding’ yields around 79% accuracy.\n",
    "\n",
    "Build a model with >90% accuracy that helps to identify between those who are looking for bootcamp style learning vs computer science majors/prospective students based on the words they use online."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c131f86",
   "metadata": {},
   "source": [
    "## Jupyter Notebook Purpose:\n",
    "#### Notebook contains codes for:\n",
    "- TF-IDF data preprocessing method.\n",
    "- Gridsearch CV (model optimization) on selected Models such as Bernoulli NB, Multinomial NB, KNN, Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0fea2e",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f85de31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import nltk\n",
    "import re\n",
    "import jupyternotify\n",
    "import string\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25c47996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b32ae8",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c8f1b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset for model training\n",
    "data = pd.read_csv('../project_3/datasets/data_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da93ef97",
   "metadata": {},
   "source": [
    "## Data Dictionary:\n",
    "\n",
    "**Dataset name: data_2**\n",
    "- Size: 9982 observations, 2 variables\n",
    "- Description: Final dataset that contains scrapped data from Reddit\n",
    "\n",
    "|Feature|Type|Dataset|Description|\n",
    "|:---|:---|:---|:---|\n",
    "|**subreddit**|*integer*|data|Subreddit categories. 0 refers to csMajors, 1 refers to codingbootcamp| \n",
    "|**text**|*string*|data|Posts extracted from csMajor and codingbootcamp subreddit|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846fa980",
   "metadata": {},
   "source": [
    "## Setting Up Data For Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beda6eb9",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b98a25d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"text\"]\n",
    "y = data[\"subreddit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba81392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385fa27a",
   "metadata": {},
   "source": [
    "## Vectorizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fe1dc5",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2ff44d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK(Natural Language Toolkit).\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "# add more stopwords\n",
    "add_stopword = stopword + stopword + [\"want\",\"one\",\"anyone\",\"like\",\"im\",\"get\",\"would\",\"got\"] + ['really','also','ive','know','dont','go','much','lot','take','think','even','getting','back',\"\"]\n",
    "\n",
    "# Lemmatizing\n",
    "wn = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6f015dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [wn.lemmatize(word) for word in tokens if word not in add_stopword]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08efa1f0",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69c3928",
   "metadata": {},
   "source": [
    "### Setting up pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ba5f7e",
   "metadata": {},
   "source": [
    "#### TF-IDF + Bernoulli Naive Bayers + Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80911fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up pipeline with 2 stages:\n",
    "# 1. TF-IDF + clean text\n",
    "# 2. Bernoulli Naive Bayes (estimator)\n",
    "\n",
    "pipe_tf_ber = Pipeline([\n",
    "    (\"tf\", TfidfVectorizer(analyzer=clean_text)),\n",
    "    ('ber', BernoulliNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dca9f943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('tf',\n",
       "   TfidfVectorizer(analyzer=<function clean_text at 0x000001C454A1BEE0>)),\n",
       "  ('ber', BernoulliNB())],\n",
       " 'verbose': False,\n",
       " 'tf': TfidfVectorizer(analyzer=<function clean_text at 0x000001C454A1BEE0>),\n",
       " 'ber': BernoulliNB(),\n",
       " 'tf__analyzer': <function __main__.clean_text(text)>,\n",
       " 'tf__binary': False,\n",
       " 'tf__decode_error': 'strict',\n",
       " 'tf__dtype': numpy.float64,\n",
       " 'tf__encoding': 'utf-8',\n",
       " 'tf__input': 'content',\n",
       " 'tf__lowercase': True,\n",
       " 'tf__max_df': 1.0,\n",
       " 'tf__max_features': None,\n",
       " 'tf__min_df': 1,\n",
       " 'tf__ngram_range': (1, 1),\n",
       " 'tf__norm': 'l2',\n",
       " 'tf__preprocessor': None,\n",
       " 'tf__smooth_idf': True,\n",
       " 'tf__stop_words': None,\n",
       " 'tf__strip_accents': None,\n",
       " 'tf__sublinear_tf': False,\n",
       " 'tf__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tf__tokenizer': None,\n",
       " 'tf__use_idf': True,\n",
       " 'tf__vocabulary': None,\n",
       " 'ber__alpha': 1.0,\n",
       " 'ber__binarize': 0.0,\n",
       " 'ber__class_prior': None,\n",
       " 'ber__fit_prior': True}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_tf_ber.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "672e0b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter tuning\n",
    "params_tf_ber = {\n",
    "    'tf__min_df': [0,1],\n",
    "    'tf__max_df': [0.4],\n",
    "    'ber__alpha': [0.3],\n",
    "    'ber__binarize' : [0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c7006a6c",
   "metadata": {},
   "source": [
    "#hyperparameter tuning (documented tested range)\n",
    "params_tf_ber = {\n",
    "    'tf__min_df': [1,2,3,4],\n",
    "    'tf__max_df': [0.4,0.6,0.8,0.9],\n",
    "    'ber__alpha': [0,0.3,0.5],\n",
    "    'ber__binarize' : [0,0.1,0.2]\n",
    "}\n",
    "\n",
    "{'ber__alpha': 0.3, 'ber__binarize': 0.1, 'tf__max_df': 0.8, 'tf__min_df': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8011487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV\n",
    "gs_tf_ber = GridSearchCV(pipe_tf_ber, # what object are we optimizing?\n",
    "                  param_grid=params_tf_ber, # what parameters values are we searching?\n",
    "                  cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da50a60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tf',\n",
       "                                        TfidfVectorizer(analyzer=<function clean_text at 0x000001C454A1BEE0>)),\n",
       "                                       ('ber', BernoulliNB())]),\n",
       "             param_grid={'ber__alpha': [0.3], 'ber__binarize': [0.1],\n",
       "                         'tf__max_df': [0.4], 'tf__min_df': [0, 1]})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit GridSearch to training data.\n",
    "gs_tf_ber.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4eb8bcc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ber__alpha': 0.3, 'ber__binarize': 0.1, 'tf__max_df': 0.4, 'tf__min_df': 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best params\n",
    "gs_tf_ber.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cf0cbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best cross validation score: 0.9051567309072974\n",
      "score on train set: 0.9569863745658562\n",
      "score on test set: 0.9254807692307693\n"
     ]
    }
   ],
   "source": [
    "# Best score: Mean cross-validated score of the best_estimator (we did a cv = 5)\n",
    "gs_tf_ber.best_score_\n",
    "\n",
    "# Score model on training set.\n",
    "gs_tf_ber.score(X_train, y_train)\n",
    "\n",
    "# Score model on testing set.\n",
    "gs_tf_ber.score(X_test, y_test)\n",
    "\n",
    "print(f'best cross validation score: {gs_tf_ber.best_score_}')\n",
    "print(f'score on train set: {gs_tf_ber.score(X_train, y_train)}')\n",
    "print(f'score on test set: {gs_tf_ber.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15a769f",
   "metadata": {},
   "source": [
    "- best cross validation score: 0.9051567309072974\n",
    "- score on train set: 0.9569863745658562\n",
    "- score on test set: 0.9254807692307693"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa47a85d",
   "metadata": {},
   "source": [
    "#### TF-IDF + Multinomial Naive Bayers + Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e0f7de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up pipeline with 2 stages:\n",
    "# 1. TF-IDF + clean text\n",
    "# 2. Multinomial Naive Bayes (estimator)\n",
    "\n",
    "pipe_tf_mnb = Pipeline([\n",
    "    (\"tf\", TfidfVectorizer(analyzer=clean_text)),\n",
    "    ('mnb', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e75a5d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('tf',\n",
       "   TfidfVectorizer(analyzer=<function clean_text at 0x000001C454A1BEE0>)),\n",
       "  ('mnb', MultinomialNB())],\n",
       " 'verbose': False,\n",
       " 'tf': TfidfVectorizer(analyzer=<function clean_text at 0x000001C454A1BEE0>),\n",
       " 'mnb': MultinomialNB(),\n",
       " 'tf__analyzer': <function __main__.clean_text(text)>,\n",
       " 'tf__binary': False,\n",
       " 'tf__decode_error': 'strict',\n",
       " 'tf__dtype': numpy.float64,\n",
       " 'tf__encoding': 'utf-8',\n",
       " 'tf__input': 'content',\n",
       " 'tf__lowercase': True,\n",
       " 'tf__max_df': 1.0,\n",
       " 'tf__max_features': None,\n",
       " 'tf__min_df': 1,\n",
       " 'tf__ngram_range': (1, 1),\n",
       " 'tf__norm': 'l2',\n",
       " 'tf__preprocessor': None,\n",
       " 'tf__smooth_idf': True,\n",
       " 'tf__stop_words': None,\n",
       " 'tf__strip_accents': None,\n",
       " 'tf__sublinear_tf': False,\n",
       " 'tf__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tf__tokenizer': None,\n",
       " 'tf__use_idf': True,\n",
       " 'tf__vocabulary': None,\n",
       " 'mnb__alpha': 1.0,\n",
       " 'mnb__class_prior': None,\n",
       " 'mnb__fit_prior': True}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_tf_mnb.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "381d7ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter tuning\n",
    "params_tf_mnb = {\n",
    "    'tf__min_df': [1],\n",
    "    'tf__max_df': [0.6],\n",
    "    'mnb__alpha': [0.3]\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c41bcbc6",
   "metadata": {},
   "source": [
    "#documented params\n",
    "params_tf_mnb = {\n",
    "    'tf__max_features': [7_000,9_000,11_000,13_000],\n",
    "    'tf__min_df': [1,2,3,4],\n",
    "    'tf__max_df': [0.6,0.8,0.9],\n",
    "    'mnb__alpha': [0,0.3,0.5]\n",
    "}\n",
    "\n",
    "{'mnb__alpha': 0.3, 'tf__max_df': 0.6, 'tf__min_df': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfac370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV\n",
    "gs_tf_mnb = GridSearchCV(pipe_tf_mnb, # what object are we optimizing?\n",
    "                  param_grid=params_tf_mnb, # what parameters values are we searching?\n",
    "                  cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "563f3be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tf',\n",
       "                                        TfidfVectorizer(analyzer=<function clean_text at 0x000001C454A1BEE0>)),\n",
       "                                       ('mnb', MultinomialNB())]),\n",
       "             param_grid={'mnb__alpha': [0.3], 'tf__max_df': [0.6],\n",
       "                         'tf__min_df': [1]})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit GridSearch to training data.\n",
    "gs_tf_mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e137577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mnb__alpha': 0.3, 'tf__max_df': 0.6, 'tf__min_df': 1}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best params:\n",
    "gs_tf_mnb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8b42848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best cross validation score: 0.9095654593566305\n",
      "score on train set: 0.9543147208121827\n",
      "score on test set: 0.9274839743589743\n"
     ]
    }
   ],
   "source": [
    "# Best score: Mean cross-validated score of the best_estimator (we did a cv = 5)\n",
    "gs_tf_mnb.best_score_\n",
    "\n",
    "# Score model on training set.\n",
    "gs_tf_mnb.score(X_train, y_train)\n",
    "\n",
    "# Score model on testing set.\n",
    "gs_tf_mnb.score(X_test, y_test)\n",
    "\n",
    "print(f'best cross validation score: {gs_tf_mnb.best_score_}')\n",
    "print(f'score on train set: {gs_tf_mnb.score(X_train, y_train)}')\n",
    "print(f'score on test set: {gs_tf_mnb.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00617703",
   "metadata": {},
   "source": [
    "- best cross validation score: 0.9095654593566305\n",
    "- score on train set: 0.9543147208121827\n",
    "- score on test set: 0.9274839743589743"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43de8b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.90      0.93      1248\n",
      "           1       0.90      0.96      0.93      1248\n",
      "\n",
      "    accuracy                           0.93      2496\n",
      "   macro avg       0.93      0.93      0.93      2496\n",
      "weighted avg       0.93      0.93      0.93      2496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_tf_mnb = gs_tf_mnb.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_tf_mnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f365fbb",
   "metadata": {},
   "source": [
    "#### TF-IDF + Logistic Regression + Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8ad7644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up pipeline with 2 stages:\n",
    "# 1. TF-IDF (transformer) + clean text\n",
    "# 2. Logistic Regression (estimator)\n",
    "\n",
    "pipe_tf_lg = Pipeline([\n",
    "    (\"tf\", TfidfVectorizer(analyzer=clean_text)),\n",
    "    ('lg', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "037ca654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('tf',\n",
       "   TfidfVectorizer(analyzer=<function clean_text at 0x000001C454A1BEE0>)),\n",
       "  ('lg', LogisticRegression())],\n",
       " 'verbose': False,\n",
       " 'tf': TfidfVectorizer(analyzer=<function clean_text at 0x000001C454A1BEE0>),\n",
       " 'lg': LogisticRegression(),\n",
       " 'tf__analyzer': <function __main__.clean_text(text)>,\n",
       " 'tf__binary': False,\n",
       " 'tf__decode_error': 'strict',\n",
       " 'tf__dtype': numpy.float64,\n",
       " 'tf__encoding': 'utf-8',\n",
       " 'tf__input': 'content',\n",
       " 'tf__lowercase': True,\n",
       " 'tf__max_df': 1.0,\n",
       " 'tf__max_features': None,\n",
       " 'tf__min_df': 1,\n",
       " 'tf__ngram_range': (1, 1),\n",
       " 'tf__norm': 'l2',\n",
       " 'tf__preprocessor': None,\n",
       " 'tf__smooth_idf': True,\n",
       " 'tf__stop_words': None,\n",
       " 'tf__strip_accents': None,\n",
       " 'tf__sublinear_tf': False,\n",
       " 'tf__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tf__tokenizer': None,\n",
       " 'tf__use_idf': True,\n",
       " 'tf__vocabulary': None,\n",
       " 'lg__C': 1.0,\n",
       " 'lg__class_weight': None,\n",
       " 'lg__dual': False,\n",
       " 'lg__fit_intercept': True,\n",
       " 'lg__intercept_scaling': 1,\n",
       " 'lg__l1_ratio': None,\n",
       " 'lg__max_iter': 100,\n",
       " 'lg__multi_class': 'auto',\n",
       " 'lg__n_jobs': None,\n",
       " 'lg__penalty': 'l2',\n",
       " 'lg__random_state': None,\n",
       " 'lg__solver': 'lbfgs',\n",
       " 'lg__tol': 0.0001,\n",
       " 'lg__verbose': 0,\n",
       " 'lg__warm_start': False}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_tf_lg.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eaec1a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter tuning\n",
    "params_tf_lg = {\n",
    "    'tf__max_features': [13_000],\n",
    "    'tf__min_df': [1],\n",
    "    'tf__max_df': [0.4],\n",
    "    'lg__solver': [\"saga\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bfef2cc1",
   "metadata": {},
   "source": [
    "#hyperparameter tuning (documented)\n",
    "params_tf_lg = {\n",
    "    'tf__max_features': [7_000,11_000,13_000,16_000]\n",
    "    'tf__min_df': [0,0.1,0.2,0.3,0.4,1,2,3,4],\n",
    "    'tf__max_df': [0,0.1,0.2,0.3,0.4,0.5,0.6,0.8,0.9],\n",
    "}\n",
    "{'lg__solver': 'saga',\n",
    " 'tf__max_df': 0.4,\n",
    " 'tf__max_features': 13000,\n",
    " 'tf__min_df': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38b2c2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV\n",
    "gs_tf_lg = GridSearchCV(pipe_tf_lg, # what object are we optimizing?\n",
    "                  param_grid=params_tf_lg, # what parameters values are we searching?\n",
    "                  cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2bcaa5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tf',\n",
       "                                        TfidfVectorizer(analyzer=<function clean_text at 0x000001C454A1BEE0>)),\n",
       "                                       ('lg', LogisticRegression())]),\n",
       "             param_grid={'lg__solver': ['saga'], 'tf__max_df': [0.4],\n",
       "                         'tf__max_features': [13000], 'tf__min_df': [1]})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit GridSearch to training data.\n",
    "gs_tf_lg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82c60f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lg__solver': 'saga',\n",
       " 'tf__max_df': 0.4,\n",
       " 'tf__max_features': 13000,\n",
       " 'tf__min_df': 1}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best params:\n",
    "gs_tf_lg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6bdb3e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best cross validation score: 0.9275977856915434\n",
      "score on train set: 0.962463264760887\n",
      "score on test set: 0.9423076923076923\n"
     ]
    }
   ],
   "source": [
    "# Best score: Mean cross-validated score of the best_estimator (we did a cv = 5)\n",
    "print(f'best cross validation score: {gs_tf_lg.best_score_}') \n",
    "\n",
    "# Score model on training set.\n",
    "print(f'score on train set: {gs_tf_lg.score(X_train, y_train)}')\n",
    "\n",
    "# Score model on testing set.\n",
    "print(f'score on test set: {gs_tf_lg.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a146600",
   "metadata": {},
   "source": [
    "- best cross validation score: 0.9270633835539348\n",
    "- score on train set: 0.9619289340101523\n",
    "- score on test set: 0.9423076923076923"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "117ccf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94      1248\n",
      "           1       0.93      0.95      0.94      1248\n",
      "\n",
      "    accuracy                           0.94      2496\n",
      "   macro avg       0.94      0.94      0.94      2496\n",
      "weighted avg       0.94      0.94      0.94      2496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_tf_lg = gs_tf_lg.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_tf_lg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4c50a5",
   "metadata": {},
   "source": [
    "#### TF-IDF + KNN + Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "738a6ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up pipeline with 2 stages:\n",
    "# 1. TF-IDF (transformer) + clean text\n",
    "# 2. KNN (estimator)\n",
    "\n",
    "pipe_tf_knn = Pipeline([\n",
    "    (\"tf\", TfidfVectorizer(analyzer=clean_text)),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "128520ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('tf',\n",
       "   TfidfVectorizer(analyzer=<function clean_text at 0x000001C454A1BEE0>)),\n",
       "  ('knn', KNeighborsClassifier())],\n",
       " 'verbose': False,\n",
       " 'tf': TfidfVectorizer(analyzer=<function clean_text at 0x000001C454A1BEE0>),\n",
       " 'knn': KNeighborsClassifier(),\n",
       " 'tf__analyzer': <function __main__.clean_text(text)>,\n",
       " 'tf__binary': False,\n",
       " 'tf__decode_error': 'strict',\n",
       " 'tf__dtype': numpy.float64,\n",
       " 'tf__encoding': 'utf-8',\n",
       " 'tf__input': 'content',\n",
       " 'tf__lowercase': True,\n",
       " 'tf__max_df': 1.0,\n",
       " 'tf__max_features': None,\n",
       " 'tf__min_df': 1,\n",
       " 'tf__ngram_range': (1, 1),\n",
       " 'tf__norm': 'l2',\n",
       " 'tf__preprocessor': None,\n",
       " 'tf__smooth_idf': True,\n",
       " 'tf__stop_words': None,\n",
       " 'tf__strip_accents': None,\n",
       " 'tf__sublinear_tf': False,\n",
       " 'tf__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tf__tokenizer': None,\n",
       " 'tf__use_idf': True,\n",
       " 'tf__vocabulary': None,\n",
       " 'knn__algorithm': 'auto',\n",
       " 'knn__leaf_size': 30,\n",
       " 'knn__metric': 'minkowski',\n",
       " 'knn__metric_params': None,\n",
       " 'knn__n_jobs': None,\n",
       " 'knn__n_neighbors': 5,\n",
       " 'knn__p': 2,\n",
       " 'knn__weights': 'uniform'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_tf_knn.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "936ec8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter tuning\n",
    "params_tf_knn = {\n",
    "    'tf__max_features': [13_000],\n",
    "    'tf__min_df': [0],\n",
    "    'tf__max_df': [0.4],\n",
    "    'knn__n_neighbors': [19]\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e3f7145",
   "metadata": {},
   "source": [
    "#hyperparameter tuning (documented)\n",
    "params_tf_knn = {\n",
    "    'tf__max_features': [7_000,11_000,13_000,16_000],\n",
    "    'tf__min_df': [0,0.5, 1],\n",
    "    'tf__max_df': [0.4,0.6,0.8,0.9],\n",
    "    'knn__n_neighbors': [3,5,7,9,11,13,15,17,19,21]\n",
    "}\n",
    "\n",
    "{'knn__n_neighbors': 19,\n",
    " 'tf__max_df': 0.4,\n",
    " 'tf__max_features': 13000,\n",
    " 'tf__min_df': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "873de0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV\n",
    "gs_tf_knn = GridSearchCV(pipe_tf_knn, # what object are we optimizing?\n",
    "                  param_grid=params_tf_knn, # what parameters values are we searching?\n",
    "                  cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07cdc605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tf',\n",
       "                                        TfidfVectorizer(analyzer=<function clean_text at 0x000001C454A1BEE0>)),\n",
       "                                       ('knn', KNeighborsClassifier())]),\n",
       "             param_grid={'knn__n_neighbors': [19], 'tf__max_df': [0.4],\n",
       "                         'tf__max_features': [13000], 'tf__min_df': [0]})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit GridSearch to training data.\n",
    "gs_tf_knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab7e4200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'knn__n_neighbors': 19,\n",
       " 'tf__max_df': 0.4,\n",
       " 'tf__max_features': 13000,\n",
       " 'tf__min_df': 0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best params:\n",
    "gs_tf_knn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d7d01380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best cross validation score: 0.8939358913643932\n",
      "score on train set: 0.8676195565054768\n",
      "score on test set: 0.8725961538461539\n"
     ]
    }
   ],
   "source": [
    "# Best score: Mean cross-validated score of the best_estimator (we did a cv = 5)\n",
    "print(f'best cross validation score: {gs_tf_knn.best_score_}') \n",
    "\n",
    "# Score model on training set.\n",
    "print(f'score on train set: {gs_tf_knn.score(X_train, y_train)}')\n",
    "\n",
    "# Score model on testing set.\n",
    "print(f'score on test set: {gs_tf_knn.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67b2982",
   "metadata": {},
   "source": [
    "- best cross validation score: 0.8939358913643932\n",
    "- score on train set: 0.8676195565054768\n",
    "- score on test set: 0.8725961538461539"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "621d27c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.76      0.86      1248\n",
      "           1       0.81      0.98      0.89      1248\n",
      "\n",
      "    accuracy                           0.87      2496\n",
      "   macro avg       0.89      0.87      0.87      2496\n",
      "weighted avg       0.89      0.87      0.87      2496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_tf_knn = gs_tf_knn.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_tf_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77a05a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"1f0d1d0d-64c2-45aa-9c50-34dbe1bab699\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"1f0d1d0d-64c2-45aa-9c50-34dbe1bab699\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%notify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d3ae55",
   "metadata": {},
   "source": [
    "## Summary on Model Accuracy\n",
    "\n",
    "|**Vectorization Method**|**Model**|**Train Results**|**Test Results**|\n",
    "|:---|:---|:---:|:---:|\n",
    "|TF-IDF|Naive Bayes - Bernoulli|0.95698|0.92548|\n",
    "|TF-IDF|Naive Bayes - Multinomial|0.95431|0.92748|\n",
    "|TF-IDF|Logistic Regression|0.96193|0.94231|\n",
    "|TF-IDF|K-Nearest Neighbours|0.86762|0.87260|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
